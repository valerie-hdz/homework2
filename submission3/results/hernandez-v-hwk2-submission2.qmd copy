---
---
title: "Homework 2"
author: "Valerie Hernandez"
format: pdf
execute:
  enabled: true
  engine: python

  pdf:
    toc: true
    toc-depth: 2
    number-sections: true
    colorlinks: true
    fig-width: 8
    fig-height: 6
    margin-left: 1in
    margin-right: 1in
    margin-top: 1in
    margin-bottom: 1in
execute:
  echo: false
  warning: false
  message: false
---

\newpage

**GitHub Repository:** git@github.com:valerie-hdz/homework2.git

---

# Part 1: Summarize the Data (2014-2019)

This section analyzes Medicare Advantage plan and service area data from 2014 through 2019.

## Question 1: Distribution of Plan Counts by County

After removing SNPs, 800-series plans, and prescription drug only plans, I analyzed the distribution of plan counts across counties over time.
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load all years of plan data
plan_data_list = []
for year in range(2014, 2020):
    df = pd.read_csv(f"../../../data/output/ma_data_{year}.csv")
    df['year'] = year
    plan_data_list.append(df)

all_plans = pd.concat(plan_data_list, ignore_index=True)

# Count plans by county and year
plan_counts = (
    all_plans
    .groupby(['fips', 'year'])
    .size()
    .reset_index(name='plan_count')
)

# Summary statistics
summary_stats = plan_counts.groupby('year')['plan_count'].describe()
print("Plan Count Distribution by Year:")
print(summary_stats.round(2))

print(f"\nOverall Statistics:")
print(f"Median plans per county: {plan_counts['plan_count'].median():.1f}")
print(f"Mean plans per county: {plan_counts['plan_count'].mean():.1f}")
print(f"75th percentile: {plan_counts['plan_count'].quantile(0.75):.1f}")
print(f"Max plans: {plan_counts['plan_count'].max():.0f}")
```
# Store values for interpretation
median_count = plan_counts['plan_count'].median()
mean_count = plan_counts['plan_count'].mean()
p75_count = plan_counts['plan_count'].quantile(0.75)
max_count = plan_counts['plan_count'].max()
min_count = plan_counts['plan_count'].min()

![Distribution of Plan Counts by County (2014-2019)](../../../data/output/question1_plan_counts_boxplot.png)

**Interpretation:** 
The number of plans appears to be sufficient for most counties but potentially too few in rural areas. While urban counties have competition with 20+ plans, some rural counties have very limited choices such as fewer than 5 plans, which could limit beneficiary options and reduce competitive pressure on pricing and quality.

\newpage

## Question 2: Distribution of Plan Bids (2014 vs 2018)

Using the landscape files and risk/rebate data to calculate plan bids, I compared the distribution of bids between 2014 and 2018.
```{python}
# Load bid data for 2014 and 2018
bid_2014 = pd.read_csv("../../../data/output/ma_data_enhanced_2014.csv")
bid_2018 = pd.read_csv("../../../data/output/ma_data_enhanced_2018.csv")

# Identify the bid column
bid_col = 'bid'

print("2014 Bid Statistics:")
stats_2014 = bid_2014[bid_col].describe()
print(stats_2014.round(2))

print("\n2018 Bid Statistics:")
stats_2018 = bid_2018[bid_col].describe()
print(stats_2018.round(2))

# Calculate key changes
mean_2014 = bid_2014[bid_col].mean()
mean_2018 = bid_2018[bid_col].mean()
median_2014 = bid_2014[bid_col].median()
median_2018 = bid_2018[bid_col].median()
std_2014 = bid_2014[bid_col].std()
std_2018 = bid_2018[bid_col].std()

print(f"\nKey Changes:")
print(f"Mean bid: ${mean_2014:.2f} (2014) → ${mean_2018:.2f} (2018), Change: ${mean_2018-mean_2014:.2f}")
print(f"Median bid: ${median_2014:.2f} → ${median_2018:.2f}, Change: ${median_2018-median_2014:.2f}")
print(f"Std dev: ${std_2014:.2f} → ${std_2018:.2f}, Change: ${std_2018-std_2014:.2f}")
```

![Distribution of Plan Bids: 2014 vs 2018](../../../data/output/bid_histograms_2014_2018.png)

**How the distribution has changed:**

The mean bid increased from approximately $798 in 2014 to $854 in 2018, representing a $56 increase. The distribution became more dispersed, with the standard deviation increasing from $178 to $195. Both distributions remain relatively symmetric with a slight right skew, but the 2018 distribution shows more high-bid outliers. The upward shift suggests an overall cost growth in Medicare Advantage, potentially driven by increasing health care costs.


\newpage

## Question 3: Average HHI Over Time

Plot the average HHI over time from 2014 through 2019. How has the HHI changed over time? To measure HHI, you'll also need to incorporate the Medicare Advantage penetration files.
```{python}
# Load HHI results
hhi_results = pd.read_csv("../../../data/output/avg_hhi_by_year.csv")

# Display key statistics
print("Average HHI by Year:")
print(hhi_results[['year', 'avg_hhi', 'median', 'std', 'n_counties']].round(2))

# Calculate change
hhi_2014 = hhi_results[hhi_results['year'] == 2014]['avg_hhi'].values[0]
hhi_2019 = hhi_results[hhi_results['year'] == 2019]['avg_hhi'].values[0]
hhi_change = hhi_2019 - hhi_2014
hhi_pct_change = (hhi_change / hhi_2014) * 100

print(f"\nChange from 2014 to 2019:")
print(f"2014 HHI: {hhi_2014:.2f}")
print(f"2019 HHI: {hhi_2019:.2f}")
print(f"Absolute change: {hhi_change:+.2f}")
print(f"Percent change: {hhi_pct_change:+.2f}%")
```

![Average HHI Over Time (2014-2019)](../../../data/output/question3_hhi.png)

**Analysis:**

The average HHI increased from 2,675 in 2014 to 2,838 in 2019 (6.1% growth). Throughout this period, the market remained "highly concentrated" (HHI ≥ 2,500).

**HHI Interpretation:**

The increasing concentration indicates fewer plans are controlling larger shares of enrollment. Despite growing MA enrollment, markets became more concentrated rather than more competitive, suggesting industry consolidation.

\newpage

## Question 4: Medicare Advantage Penetration Over Time

Plot the average share of Medicare Advantage over time from 2014 to 2019. Has MA increased or decreased in popularity?
```{python}
# Load MA penetration results
ma_results = pd.read_csv("../../../data/output/ma_share_by_year.csv")

# Display results
print("MA Penetration by Year:")
print(ma_results[['year', 'ma_share', 'total_eligibles', 'total_enrolled']].round(2))

# Calculate change
ma_2014 = ma_results[ma_results['year'] == 2014]['ma_share'].values[0]
ma_2019 = ma_results[ma_results['year'] == 2019]['ma_share'].values[0]
ma_change = ma_2019 - ma_2014
ma_relative_change = (ma_change / ma_2014) * 100

print(f"\nChange from 2014 to 2019:")
print(f"2014: {ma_2014:.2f}%")
print(f"2019: {ma_2019:.2f}%")
print(f"Absolute change: {ma_change:+.2f} percentage points")
print(f"Relative growth: {ma_relative_change:+.2f}%")
```

![Medicare Advantage Penetration Rate (2014-2019)](../../../data/output/question4_ma_share.png)

**Has Medicare Advantage increased or decreased in popularity?**

Medicare Advantage has increased in popularity. Enrollment grew from 30.5% in 2014 to 36.8% in 2019, a 6.3 percentage point increase. This substantial growth demonstrates beneficiaries are increasingly choosing MA plans over traditional Medicare.


\newpage

# Part 2: Estimate Average Treatment Effects (2018 Only)

```{python}
import pandas as pd
import numpy as np

# 1. Filter county_df for 2018 (assuming you have this already)
county_df = county_df[county_df["year"] == 2018].copy()

# 2. Load FFS Excel file — corrected path
ffs_raw = pd.read_excel(
    "../../../data/input/ffs_2018/FFS18.xlsx",  # <-- corrected
    skiprows=2,
    names=[
        "ssa","state","county_name",
        "parta_enroll","parta_reimb","parta_percap",
        "parta_reimb_unadj","parta_percap_unadj",
        "parta_ime","parta_dsh","parta_gme",
        "partb_enroll","partb_reimb","partb_percap"
    ],
    na_values="*"
)

# 3. Clean FFS data
final_ffs_costs = (
    ffs_raw[
        ["ssa", "state", "county_name",
         "parta_enroll", "parta_reimb", "parta_percap",
         "parta_reimb_unadj", "parta_percap_unadj",
         "parta_ime", "parta_dsh", "parta_gme",
         "partb_enroll", "partb_reimb", "partb_percap"]
    ]
    .dropna(subset=["ssa"])
)

# 4. Merge on SSA + Year
county_df["ssa"] = pd.to_numeric(county_df["ssa"], errors="coerce")
county_df["year"] = 2018

final_ffs_costs["ssa"] = pd.to_numeric(final_ffs_costs["ssa"], errors="coerce")
final_ffs_costs["year"] = 2018

county_df = county_df.merge(
    final_ffs_costs,
    on=["ssa","year"],
    how="left"
)

county_df.head()
```

## Question 5: Average Bid by among Competitive vs Uncompetitive Markets
```{python}
import numpy as np
import pandas as pd

# 33rd and 66th percentile cutoffs
q_low  = county_df["hhi"].quantile(0.33)
q_high = county_df["hhi"].quantile(0.66)

county_df["treated_dummy"] = np.where(
    county_df["hhi"] >= q_high, 1,
    np.where(county_df["hhi"] <= q_low, 0, np.nan)
)

county_df = county_df.dropna(subset=["treated_dummy"]).copy()
county_df["treated_dummy"] = county_df["treated_dummy"].astype(int)

# Average bid by group
avg_bid_by_group = county_df.groupby("treated_dummy")["avg_bid"].mean()
avg_bid_by_group

\newpage

## Question 6: Average Bid by Treatment and FFS Quartile
```{python}
# Total FFS spending
county_df["ffs_total"] = (
    county_df["parta_reimb"] +
    county_df["partb_reimb"]
)

# Create quartiles
county_df["ffs_q"] = pd.qcut(
    county_df["ffs_total"],
    4,
    labels=False
) + 1

# Indicator variables
for q in [1,2,3,4]:
    county_df[f"ffs_q{q}"] = (county_df["ffs_q"] == q).astype(int)

# Table of average bids by quartile and treatment
quartile_table = (
    county_df
    .groupby(["ffs_q","treated_dummy"])["avg_bid"]
    .mean()
    .unstack()
)

quartile_table

\newpage

## Question 7: Average Treatment Effect Estimates using the listed estimators.

cov_cols = ["ffs_q1","ffs_q2","ffs_q3","ffs_q4"]

lp_df = county_df.dropna(
    subset=["avg_bid","treated_dummy"] + cov_cols
).copy()

NN  INV
from sklearn.neighbors import NearestNeighbors

treated  = lp_df[lp_df["treated_dummy"] == 1]
controls = lp_df[lp_df["treated_dummy"] == 0]

X_t = treated[cov_cols].values
X_c = controls[cov_cols].values

y_t = treated["avg_bid"].values
y_c = controls["avg_bid"].values

nn = NearestNeighbors(n_neighbors=1, metric="euclidean")
nn.fit(X_c)

_, indices = nn.kneighbors(X_t)

matched_ctrl = y_c[indices.flatten()]

ate_nn = np.mean(y_t - matched_ctrl)
ate_nn

# Mahalanobis
import numpy as np

V = np.cov(np.vstack([X_t, X_c]).T)

nn_md = NearestNeighbors(
    n_neighbors=1,
    metric="mahalanobis",
    metric_params={"V": V}
)

nn_md.fit(X_c)

_, indices = nn_md.kneighbors(X_t)

matched_ctrl = y_c[indices.flatten()]

ate_mahal = np.mean(y_t - matched_ctrl)
ate_mahal

# Inverse Propensity Weighting
import statsmodels.formula.api as smf

# Drop one quartile dummy to avoid multicollinearity
ps_model = smf.logit(
    "treated_dummy ~ ffs_q2 + ffs_q3 + ffs_q4",
    data=lp_df
).fit()

lp_df["ps"] = ps_model.predict(lp_df)
lp_df["ps"] = lp_df["ps"].clip(1e-6, 1-1e-6)

lp_df["ipw"] = np.where(
    lp_df["treated_dummy"] == 1,
    1 / lp_df["ps"],
    1 / (1 - lp_df["ps"])
)

treated  = lp_df[lp_df["treated_dummy"] == 1]
controls = lp_df[lp_df["treated_dummy"] == 0]

mean_t = np.average(treated["avg_bid"], weights=treated["ipw"])
mean_c = np.average(controls["avg_bid"], weights=controls["ipw"])

ate_ipw = mean_t - mean_c
ate_ipw

# OLS Regression
# Separate regressions
reg1_q = smf.ols(
    "avg_bid ~ ffs_q2 + ffs_q3 + ffs_q4",
    data=lp_df[lp_df["treated_dummy"]==1]
).fit()

reg0_q = smf.ols(
    "avg_bid ~ ffs_q2 + ffs_q3 + ffs_q4",
    data=lp_df[lp_df["treated_dummy"]==0]
).fit()

pred1_q = reg1_q.predict(lp_df)
pred0_q = reg0_q.predict(lp_df)

ate_quartile_ols = np.mean(pred1_q - pred0_q)
ate_quartile_ols

# Comparison Table
results = pd.DataFrame({
    "Estimator": [
        "NN Euclidean",
        "NN Mahalanobis",
        "IPW",
        "OLS (Quartile)"
    ],
    "ATE": [
        ate_nn,
        ate_mahal,
        ate_ipw,
        ate_quartile_ols
    ]
})

results

![Comparison of ATE Estimates](../../../data/output/question4_ate_comparison.png)

**Results:**

\newpage

## Question 8: Are the Results Similar Across Estimators?
```{python}
The four estimators — nearest neighbor matching (inverse variance and Mahalanobis), inverse propensity weighting (IPW), and linear regression — produce very similar estimated average treatment effects (ATEs). 

While the numerical values are not perfectly identical, the differences are very small and arise from minor weighting and functional form differences across methods. Substantively, the conclusions are the same across estimators.

Therefore, the results are highly consistent and not meaningfully different.

`\newpage

## Question 9: OLS with Continuous Covariates
cov_cont = ["ffs_total", "avg_eligibles"]

lp_df2 = county_df.dropna(
    subset=["avg_bid","treated_dummy"] + cov_cont
).copy()

# Separate regressions
reg1 = smf.ols(
    "avg_bid ~ ffs_total + avg_eligibles",
    data=lp_df2[lp_df2["treated_dummy"]==1]
).fit()

reg0 = smf.ols(
    "avg_bid ~ ffs_total + avg_eligibles",
    data=lp_df2[lp_df2["treated_dummy"]==0]
).fit()

pred1 = reg1.predict(lp_df2)
pred0 = reg0.predict(lp_df2)

ate_continuous_ols = np.mean(pred1 - pred0)

ate_continuous_ols

Interpretation: Using OLS with continuous FFS costs and Medicare beneficiaries yields an estimated treatment effect of 8.45, 
compared to 8.44 when using quartile indicators. The results are broadly similar, suggesting that discretizing FFS costs into 
quartiles does not materially change the estimated treatment effect. Minor differences reflect the additional 
flexibility gained from using continuous covariates.

\newpage

## Question 10: Reflection

One thing I learned is how to go about integrating multiple datasets across different geographic identifiers. One thing that was challenging for me, was deciding how I would go about organizing my workflow and creating the 2014-2019.

**Repository:** git@github.com:valerie-hdz/homework2.git
