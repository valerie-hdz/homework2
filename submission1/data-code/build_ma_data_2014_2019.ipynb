{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3dd0aa",
   "metadata": {},
   "source": [
    "## Build Medicare Advantage Data (2014-2019)\n",
    "### Code to build plan and service area data for each year from 2014 to 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d08f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# settings\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# Paths \n",
    "BASE = Path(\"../../..\")\n",
    "OUT_DIR = BASE /\"data\" / \"output\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c63df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_colnames(cols):\n",
    "    \"\"\"Standardize column names: lower-case, strip, replace spaces.punct with _.\"\"\"\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        c2 = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", str(c).strip()).strip(\"_\").lower()\n",
    "        out.append(c2)\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_contract(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a single monthly Contract Info file.\"\"\"\n",
    "    df = pd.read_csv(path, dtype=str, encoding=\"latin1\")\n",
    "    df.columns = _clean_colnames(df.columns)\n",
    "\n",
    "    # Harmonize common column-name variants\n",
    "    ren = {\n",
    "        \"contract_id\": \"contractid\",\n",
    "        \"contract\":    \"contractid\",\n",
    "        \"plan_id\":     \"planid\",\n",
    "        \"plan\":        \"planid\",\n",
    "        \"organization_type\": \"org_type\",\n",
    "        \"plan_type\":   \"plan_type\",\n",
    "        \"offers_part_d\": \"partd\",\n",
    "        \"snp_plan\":    \"snp\",\n",
    "        \"organization_name\": \"org_name\",\n",
    "        \"organization_marketing_name\": \"org_marketing_name\",\n",
    "        \"parent_organization\": \"parent_org\",\n",
    "        \"contract_effective_date\": \"contract_date\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
    "\n",
    "    if \"planid\" in df.columns:\n",
    "        df[\"planid\"] = pd.to_numeric(df[\"planid\"], errors=\"coerce\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3228c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_enroll(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a single monthly Enrollment Info file.\"\"\"\n",
    "    df = pd.read_csv(path, dtype=str, na_values=[\"*\"], encoding=\"latin1\")\n",
    "    df.columns = _clean_colnames(df.columns)\n",
    "\n",
    "    # Harmonize common column-name variants\n",
    "    ren = {\n",
    "        \"contract_number\": \"contractid\",\n",
    "        \"contract_id\":     \"contractid\",\n",
    "        \"contract\":        \"contractid\",\n",
    "        \"plan_id\":         \"planid\",\n",
    "        \"plan\":            \"planid\",\n",
    "        \"county_name\":     \"county\",\n",
    "        \"state_abbr\":      \"state\",\n",
    "        \"fips_state_county_code\": \"fips\",\n",
    "        \"ssa_state_county_code\":  \"ssa\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
    "\n",
    "    if \"planid\" in df.columns:\n",
    "        df[\"planid\"] = pd.to_numeric(df[\"planid\"], errors=\"coerce\")\n",
    "    if \"fips\" in df.columns:\n",
    "        df[\"fips\"] = pd.to_numeric(df[\"fips\"], errors=\"coerce\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e29583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_service_area(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a single monthly service-area file.\"\"\"\n",
    "    df = pd.read_csv(path, dtype=str, na_values=[\"*\"], encoding=\"latin1\")\n",
    "    df.columns = _clean_colnames(df.columns)\n",
    "\n",
    "    # Harmonize common column-name variants\n",
    "    ren = {\n",
    "        \"contract_id\":        \"contractid\",\n",
    "        \"organization_name\":  \"org_name\",\n",
    "        \"organization_type\":  \"org_type\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
    "\n",
    "    # Type fixes\n",
    "    if \"partial\" in df.columns:\n",
    "        df[\"partial\"] = df[\"partial\"].map(\n",
    "            lambda x: str(x).strip().lower() in {\"true\", \"t\", \"1\", \"yes\", \"y\"}\n",
    "            if pd.notna(x) else np.nan\n",
    "        )\n",
    "    for col in [\"ssa\", \"fips\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_month(enroll_dir: Path, m: str, y: int) -> pd.DataFrame:\n",
    "    \"\"\"Load contract + enrollment for one month and merge them.\"\"\"\n",
    "    c_path = enroll_dir / f\"CPSC_Contract_Info_{y}_{m}.csv\"\n",
    "    e_path = enroll_dir / f\"CPSC_Enrollment_Info_{y}_{m}.csv\"\n",
    "\n",
    "    contract = read_contract(c_path)\n",
    "\n",
    "    # Keep one row per (contractid, planid)\n",
    "    if {\"contractid\", \"planid\"}.issubset(contract.columns):\n",
    "        contract = contract.drop_duplicates(subset=[\"contractid\", \"planid\"], keep=\"first\")\n",
    "\n",
    "    enroll = read_enroll(e_path)\n",
    "\n",
    "    # Merge contract info into enrollment\n",
    "    if {\"contractid\", \"planid\"}.issubset(enroll.columns) and {\"contractid\", \"planid\"}.issubset(contract.columns):\n",
    "        df = enroll.merge(contract, on=[\"contractid\", \"planid\"], how=\"left\", suffixes=(\"\", \"_contract\"))\n",
    "    else:\n",
    "        df = enroll.copy()\n",
    "\n",
    "    df[\"month\"] = int(m)\n",
    "    df[\"year\"]  = int(y)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_month_sa(sa_dir: Path, m: str, y: int) -> pd.DataFrame:\n",
    "    \"\"\"Load one month of service-area data.\"\"\"\n",
    "    path = sa_dir / f\"MA_Cnty_SA_{y}_{m}.csv\"\n",
    "    df   = read_service_area(path)\n",
    "    df[\"month\"] = int(m)\n",
    "    df[\"year\"]  = int(y)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6922b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building data for one Year\n",
    "def build_year_ma(year: int) -> pd.DataFrame:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Building MA data for year {year}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    MONTHS = [f\"{m:02d}\" for m in range(1, 13)]\n",
    "    \n",
    "    enroll_dir = BASE / \"data\" / \"input\" / f\"enrollment_{year}\"\n",
    "    sa_dir = BASE / \"data\" / \"input\" / f\"service_area_{year}\"\n",
    "    \n",
    "    if not enroll_dir.exists():\n",
    "        print(f\"ERROR: Enrollment directory not found: {enroll_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    if not sa_dir.exists():\n",
    "        print(f\"ERROR: Service area directory not found: {sa_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Read enrollment data\n",
    "    print(f\"  Reading enrollment data for {year}...\")\n",
    "    plan_year = pd.concat([load_month(enroll_dir, m, year) for m in MONTHS], ignore_index=True)\n",
    "\n",
    "    sort_cols = [c for c in [\"contractid\", \"planid\", \"state\", \"county\", \"month\"] if c in plan_year.columns]\n",
    "    plan_year = plan_year.sort_values(sort_cols).reset_index(drop=True)\n",
    "    \n",
    "    # Fill missing FIPS\n",
    "    if {\"state\", \"county\", \"fips\"}.issubset(plan_year.columns):\n",
    "        plan_year[\"fips\"] = (\n",
    "            plan_year.groupby([\"state\", \"county\"], dropna=False)[\"fips\"]\n",
    "            .transform(lambda s: s.ffill().bfill())\n",
    "        )\n",
    "    \n",
    "    # Fill plan-level descriptors\n",
    "    fill_cols_planid = [c for c in [\"plan_type\", \"partd\", \"snp\", \"eghp\", \"plan_name\"] if c in plan_year.columns]\n",
    "    if fill_cols_planid and {\"contractid\", \"planid\"}.issubset(plan_year.columns):\n",
    "        plan_year[fill_cols_planid] = (\n",
    "            plan_year.groupby([\"contractid\", \"planid\"], dropna=False)[fill_cols_planid]\n",
    "            .transform(lambda df: df.ffill().bfill())\n",
    "        )\n",
    "# Fill org-level descriptors\n",
    "    fill_cols_contract = [c for c in [\"org_name\", \"org_marketing_name\", \"org_type\", \"parent_org\", \"contract_date\"] \n",
    "                          if c in plan_year.columns]\n",
    "    if fill_cols_contract and \"contractid\" in plan_year.columns:\n",
    "        plan_year[fill_cols_contract] = (\n",
    "            plan_year.groupby([\"contractid\"], dropna=False)[fill_cols_contract]\n",
    "            .transform(lambda df: df.ffill().bfill())\n",
    "        )\n",
    "    \n",
    "    # Collapse to one row per (contractid, planid, fips, year)\n",
    "    group_cols = [c for c in [\"contractid\", \"planid\", \"fips\", \"year\"] if c in plan_year.columns]\n",
    "    plan_year = plan_year.sort_values([*group_cols, \"month\"]).reset_index(drop=True)\n",
    "    \n",
    "    carry_cols = [c for c in [\n",
    "        \"state\", \"county\", \"org_type\", \"plan_type\", \"partd\", \"snp\", \"eghp\",\n",
    "        \"org_name\", \"org_marketing_name\", \"plan_name\", \"parent_org\", \"contract_date\",\n",
    "    ] if c in plan_year.columns]\n",
    "    \n",
    "    enroll_like = [c for c in plan_year.columns\n",
    "                   if any(k in c for k in [\"enroll\", \"enrollment\", \"benefici\", \"member\", \"eligible\"])]\n",
    "    carry_cols += [c for c in enroll_like if c not in carry_cols and c not in group_cols and c != \"month\"]\n",
    "    \n",
    "    final_plans = (\n",
    "        plan_year.groupby(group_cols, dropna=False, as_index=False)\n",
    "        .agg({c: \"last\" for c in carry_cols})\n",
    "    )\n",
    "    \n",
    "    print(f\"  Plan data: {len(final_plans):,} rows\")\n",
    "\n",
    "    # Read service-area data\n",
    "    print(f\"  Reading service area data for {year}...\")\n",
    "    service_year = pd.concat([load_month_sa(sa_dir, m, year) for m in MONTHS], ignore_index=True)\n",
    "    \n",
    "    sort_cols = [c for c in [\"contractid\", \"fips\", \"state\", \"county\", \"month\"] if c in service_year.columns]\n",
    "    service_year = service_year.sort_values(sort_cols).reset_index(drop=True)\n",
    "    \n",
    "    # Fill missing fips\n",
    "    if {\"state\", \"county\", \"fips\"}.issubset(service_year.columns):\n",
    "        service_year[\"fips\"] = (\n",
    "            service_year.groupby([\"state\", \"county\"], dropna=False)[\"fips\"]\n",
    "            .transform(lambda s: s.ffill().bfill())\n",
    "        )\n",
    "    \n",
    "    # Fill labels\n",
    "    fill_cols_sa = [c for c in [\"org_name\", \"org_type\", \"plan_type\", \"partial\", \"eghp\", \"ssa\", \"notes\"]\n",
    "                    if c in service_year.columns]\n",
    "    if fill_cols_sa and \"contractid\" in service_year.columns:\n",
    "        service_year[fill_cols_sa] = (\n",
    "            service_year.groupby([\"contractid\"], dropna=False)[fill_cols_sa]\n",
    "            .transform(lambda df: df.ffill().bfill())\n",
    "        )\n",
    "    \n",
    "    # Collapse service area\n",
    "    group_cols_sa = [c for c in [\"contractid\", \"fips\", \"year\"] if c in service_year.columns]\n",
    "    service_year = service_year.sort_values([*group_cols_sa, \"month\"]).reset_index(drop=True)\n",
    "    \n",
    "    carry_cols_sa = [c for c in [\"state\", \"county\", \"org_name\", \"org_type\", \"plan_type\",\n",
    "                                  \"partial\", \"eghp\", \"ssa\", \"notes\"]\n",
    "                     if c in service_year.columns]\n",
    "    \n",
    "    final_service_area = (\n",
    "        service_year.groupby(group_cols_sa, dropna=False, as_index=False)\n",
    "        .agg({c: \"last\" for c in carry_cols_sa})\n",
    "    )\n",
    "    \n",
    "    print(f\"  Service area data: {len(final_service_area):,} rows\")\n",
    "    \n",
    "    # Merge\n",
    "    print(f\"  Merging plan and service area data...\")\n",
    "    merged = final_plans.merge(\n",
    "        final_service_area,\n",
    "        on=[\"contractid\", \"fips\", \"year\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_plan\", \"_sa\"),\n",
    "    )\n",
    "    \n",
    "    print(f\"  Merged data (before filters): {len(merged):,} rows\")\n",
    "    \n",
    "    # Apply filters\n",
    "    planid_num = pd.to_numeric(merged.get(\"planid\"), errors=\"coerce\")\n",
    "    \n",
    "    snp_col = \"snp\" if \"snp\" in merged.columns else None\n",
    "    plan_type_col = \"plan_type_plan\" if \"plan_type_plan\" in merged.columns else \"plan_type\"\n",
    "    \n",
    "    filtered = merged.copy()\n",
    "    \n",
    "    # Remove SNPs\n",
    "    if snp_col:\n",
    "        filtered = filtered[filtered[snp_col] == \"No\"]\n",
    "        print(f\"  After removing SNPs: {len(filtered):,} rows\")\n",
    "    \n",
    "    # Remove 800-series\n",
    "    filtered = filtered[(planid_num < 800) | (planid_num >= 900)]\n",
    "    print(f\"  After removing 800-series: {len(filtered):,} rows\")\n",
    "    \n",
    "    # Remove drug-only plans\n",
    "    if plan_type_col in filtered.columns:\n",
    "        filtered[\"is_pdp_only\"] = filtered[plan_type_col].astype(str).str.contains(\n",
    "            r\"(?i)(prescription|^pdp|drug plan)\", \n",
    "            na=False, \n",
    "            regex=True\n",
    "        ) & ~filtered[plan_type_col].astype(str).str.contains(\n",
    "            r\"(?i)(ma-pd|hmo|ppo|pffs|msa)\", \n",
    "            na=False, \n",
    "            regex=True\n",
    "        )\n",
    "        filtered = filtered[~filtered[\"is_pdp_only\"]]\n",
    "        filtered = filtered.drop(columns=[\"is_pdp_only\"])\n",
    "        print(f\"  After removing drug-only plans: {len(filtered):,} rows\")\n",
    "    \n",
    "    print(f\"  Final filtered data: {len(filtered):,} rows\")\n",
    "    \n",
    "     # Save to file\n",
    "    out_path = OUT_DIR / f\"ma_data_{year}.csv\"\n",
    "    filtered.to_csv(out_path, index=False)\n",
    "    print(f\"  Saved to: {out_path}\")\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7bf6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_years(years: list = [2014, 2015, 2016, 2017, 2018, 2019], save: bool = True):\n",
    "    results = {}\n",
    "    \n",
    "    for year in years:\n",
    "        try:\n",
    "            df = build_year_ma(year)\n",
    "            results[year] = df\n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ ERROR building data for {year}: {e}\")\n",
    "            results[year] = pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Summary:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for year, df in results.items():\n",
    "        status = \"✓\" if not df.empty else \"✗\"\n",
    "        print(f\"{status} {year}: {len(df):,} rows\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "000b862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use:\n",
    "# build_single_year(2018, save=True)  # Build one year\n",
    "# build_all_years(years=[2014, 2015, 2016, 2017, 2018, 2019], save=True)  # Build all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "360e0ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in output directory:\n",
      "============================================================\n",
      "✗ No ma_data_*.csv files found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"../../../data/output\")\n",
    "\n",
    "print(\"Files in output directory:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if OUT_DIR.exists():\n",
    "    files = list(OUT_DIR.glob(\"ma_data_*.csv\"))\n",
    "    if files:\n",
    "        for f in sorted(files):\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"✓ {f.name} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(\"✗ No ma_data_*.csv files found\")\n",
    "else:\n",
    "    print(f\"✗ Directory not found: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "154f8d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building MA data for year 2014\n",
      "============================================================\n",
      "  Reading enrollment data for 2014...\n",
      "  Plan data: 2,533,855 rows\n",
      "  Reading service area data for 2014...\n",
      "  Service area data: 397,162 rows\n",
      "  Merging plan and service area data...\n",
      "  Merged data (before filters): 1,222,286 rows\n",
      "  After removing SNPs: 1,211,858 rows\n",
      "  After removing 800-series: 65,095 rows\n",
      "  After removing drug-only plans: 65,095 rows\n",
      "  Final filtered data: 65,095 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:132: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered = filtered[(planid_num < 800) | (planid_num >= 900)]\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:137: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered[\"is_pdp_only\"] = filtered[plan_type_col].astype(str).str.contains(\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:141: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ) & ~filtered[plan_type_col].astype(str).str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved to: ../../../data/output/ma_data_2014.csv\n",
      "\n",
      "============================================================\n",
      "Building MA data for year 2015\n",
      "============================================================\n",
      "  Reading enrollment data for 2015...\n",
      "  Plan data: 2,388,316 rows\n",
      "  Reading service area data for 2015...\n",
      "  Service area data: 377,761 rows\n",
      "  Merging plan and service area data...\n",
      "  Merged data (before filters): 1,213,247 rows\n",
      "  After removing SNPs: 1,202,216 rows\n",
      "  After removing 800-series: 67,505 rows\n",
      "  After removing drug-only plans: 67,505 rows\n",
      "  Final filtered data: 67,505 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:132: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered = filtered[(planid_num < 800) | (planid_num >= 900)]\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:137: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered[\"is_pdp_only\"] = filtered[plan_type_col].astype(str).str.contains(\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:141: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ) & ~filtered[plan_type_col].astype(str).str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved to: ../../../data/output/ma_data_2015.csv\n",
      "\n",
      "============================================================\n",
      "Building MA data for year 2016\n",
      "============================================================\n",
      "  Reading enrollment data for 2016...\n",
      "  Plan data: 2,154,444 rows\n",
      "  Reading service area data for 2016...\n",
      "  Service area data: 324,663 rows\n",
      "  Merging plan and service area data...\n",
      "  Merged data (before filters): 1,059,738 rows\n",
      "  After removing SNPs: 1,047,344 rows\n",
      "  After removing 800-series: 70,772 rows\n",
      "  After removing drug-only plans: 70,772 rows\n",
      "  Final filtered data: 70,772 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:132: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered = filtered[(planid_num < 800) | (planid_num >= 900)]\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:137: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered[\"is_pdp_only\"] = filtered[plan_type_col].astype(str).str.contains(\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:141: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ) & ~filtered[plan_type_col].astype(str).str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved to: ../../../data/output/ma_data_2016.csv\n",
      "\n",
      "============================================================\n",
      "Building MA data for year 2017\n",
      "============================================================\n",
      "  Reading enrollment data for 2017...\n",
      "  Plan data: 2,250,310 rows\n",
      "  Reading service area data for 2017...\n",
      "  Service area data: 320,202 rows\n",
      "  Merging plan and service area data...\n",
      "  Merged data (before filters): 1,219,685 rows\n",
      "  After removing SNPs: 1,206,986 rows\n",
      "  After removing 800-series: 72,733 rows\n",
      "  After removing drug-only plans: 72,733 rows\n",
      "  Final filtered data: 72,733 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:132: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered = filtered[(planid_num < 800) | (planid_num >= 900)]\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:137: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered[\"is_pdp_only\"] = filtered[plan_type_col].astype(str).str.contains(\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:141: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ) & ~filtered[plan_type_col].astype(str).str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved to: ../../../data/output/ma_data_2017.csv\n",
      "\n",
      "============================================================\n",
      "Building MA data for year 2018\n",
      "============================================================\n",
      "  Reading enrollment data for 2018...\n",
      "  Plan data: 2,475,109 rows\n",
      "  Reading service area data for 2018...\n",
      "  Service area data: 331,593 rows\n",
      "  Merging plan and service area data...\n",
      "  Merged data (before filters): 1,366,535 rows\n",
      "  After removing SNPs: 1,350,940 rows\n",
      "  After removing 800-series: 87,708 rows\n",
      "  After removing drug-only plans: 87,708 rows\n",
      "  Final filtered data: 87,708 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:132: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered = filtered[(planid_num < 800) | (planid_num >= 900)]\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:137: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered[\"is_pdp_only\"] = filtered[plan_type_col].astype(str).str.contains(\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:141: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ) & ~filtered[plan_type_col].astype(str).str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved to: ../../../data/output/ma_data_2018.csv\n",
      "\n",
      "============================================================\n",
      "Building MA data for year 2019\n",
      "============================================================\n",
      "  Reading enrollment data for 2019...\n",
      "  Plan data: 2,875,213 rows\n",
      "  Reading service area data for 2019...\n",
      "  Service area data: 345,827 rows\n",
      "  Merging plan and service area data...\n",
      "  Merged data (before filters): 1,693,175 rows\n",
      "  After removing SNPs: 1,675,403 rows\n",
      "  After removing 800-series: 97,772 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:132: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered = filtered[(planid_num < 800) | (planid_num >= 900)]\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:137: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered[\"is_pdp_only\"] = filtered[plan_type_col].astype(str).str.contains(\n",
      "/var/folders/dr/bz36x93s5vs6lnjjq8z60zfm0000gp/T/ipykernel_69427/1863573618.py:141: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ) & ~filtered[plan_type_col].astype(str).str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After removing drug-only plans: 97,772 rows\n",
      "  Final filtered data: 97,772 rows\n",
      "  Saved to: ../../../data/output/ma_data_2019.csv\n",
      "\n",
      "============================================================\n",
      "Summary:\n",
      "============================================================\n",
      "✓ 2014: 65,095 rows\n",
      "✓ 2015: 67,505 rows\n",
      "✓ 2016: 70,772 rows\n",
      "✓ 2017: 72,733 rows\n",
      "✓ 2018: 87,708 rows\n",
      "✓ 2019: 97,772 rows\n"
     ]
    }
   ],
   "source": [
    "# Call the function to build all years\n",
    "results = build_all_years(years=[2014, 2015, 2016, 2017, 2018, 2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98dd01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
