{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "add7fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-doing Question 7 of Homework 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa906d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis sample size: 1976\n",
      "Treated (High HHI): 988\n",
      "Control (Low HHI): 988\n"
     ]
    }
   ],
   "source": [
    "# Load county-level data\n",
    "county_2018 = pd.read_csv(\"../../../data/output/county_analysis_2018.csv\")\n",
    "\n",
    "# Drop any rows with missing values in key variables\n",
    "analysis_data = county_2018.dropna(subset=['bid', 'treatment', 'ffs_q2', 'ffs_q3', 'ffs_q4']).copy()\n",
    "\n",
    "print(f\"Analysis sample size: {len(analysis_data)}\")\n",
    "print(f\"Treated (High HHI): {(analysis_data['treatment']==1).sum()}\")\n",
    "print(f\"Control (Low HHI): {(analysis_data['treatment']==0).sum()}\")\n",
    "\n",
    "# Define variables\n",
    "Y = analysis_data['bid'].values  # Outcome: plan bids\n",
    "Tr = analysis_data['treatment'].values  # Treatment: 1=High HHI, 0=Low HHI\n",
    "X = analysis_data[['ffs_q2', 'ffs_q3', 'ffs_q4']].values  # Covariates: FFS quartiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1c1a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. INV (Inverse Variance Matching): $15.95\n"
     ]
    }
   ],
   "source": [
    "# Estimator 1: Nearest Neighbor Matching - Inverse Variance (ATE)\n",
    "\n",
    "def nn_matching_inv_ate(Y, Tr, X):\n",
    "\n",
    "    treated_idx = np.where(Tr == 1)[0]\n",
    "    control_idx = np.where(Tr == 0)[0]\n",
    "\n",
    "    variances = np.var(X, axis=0)\n",
    "    variances[variances == 0] = 1e-10\n",
    "    X_scaled = X / np.sqrt(variances)\n",
    "\n",
    "    # Match treated → control\n",
    "    nbrs_control = NearestNeighbors(n_neighbors=1).fit(X_scaled[control_idx])\n",
    "    _, indices_tc = nbrs_control.kneighbors(X_scaled[treated_idx])\n",
    "    matched_control = control_idx[indices_tc.flatten()]\n",
    "\n",
    "    # Match control → treated\n",
    "    nbrs_treated = NearestNeighbors(n_neighbors=1).fit(X_scaled[treated_idx])\n",
    "    _, indices_ct = nbrs_treated.kneighbors(X_scaled[control_idx])\n",
    "    matched_treated = treated_idx[indices_ct.flatten()]\n",
    "\n",
    "    Y1_hat = np.zeros(len(Y))\n",
    "    Y0_hat = np.zeros(len(Y))\n",
    "\n",
    "    Y1_hat[treated_idx] = Y[treated_idx]\n",
    "    Y0_hat[treated_idx] = Y[matched_control]\n",
    "\n",
    "    Y0_hat[control_idx] = Y[control_idx]\n",
    "    Y1_hat[control_idx] = Y[matched_treated]\n",
    "\n",
    "    ate = np.mean(Y1_hat - Y0_hat)\n",
    "\n",
    "    return ate\n",
    "\n",
    "\n",
    "ate_inv = nn_matching_inv_ate(Y, Tr, X)\n",
    "print(f\"1. INV (Inverse Variance Matching): ${ate_inv:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12bd84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. MAH (Mahalanobis Matching): $24.36\n"
     ]
    }
   ],
   "source": [
    "# Estimator 2: Nearest Neighbor Matching - Mahalanobis (ATE)\n",
    "\n",
    "def nn_matching_maha_ate(Y, Tr, X):\n",
    "\n",
    "    treated_idx = np.where(Tr == 1)[0]\n",
    "    control_idx = np.where(Tr == 0)[0]\n",
    "\n",
    "    cov_matrix = np.cov(X.T)\n",
    "    inv_cov = np.linalg.pinv(cov_matrix)\n",
    "\n",
    "    # Match treated → control\n",
    "    matched_control = []\n",
    "    for i in treated_idx:\n",
    "        dists = [mahalanobis(X[i], X[j], inv_cov) for j in control_idx]\n",
    "        matched_control.append(control_idx[np.argmin(dists)])\n",
    "\n",
    "    # Match control → treated\n",
    "    matched_treated = []\n",
    "    for i in control_idx:\n",
    "        dists = [mahalanobis(X[i], X[j], inv_cov) for j in treated_idx]\n",
    "        matched_treated.append(treated_idx[np.argmin(dists)])\n",
    "\n",
    "    Y1_hat = np.zeros(len(Y))\n",
    "    Y0_hat = np.zeros(len(Y))\n",
    "\n",
    "    Y1_hat[treated_idx] = Y[treated_idx]\n",
    "    Y0_hat[treated_idx] = Y[matched_control]\n",
    "\n",
    "    Y0_hat[control_idx] = Y[control_idx]\n",
    "    Y1_hat[control_idx] = Y[matched_treated]\n",
    "\n",
    "    ate = np.mean(Y1_hat - Y0_hat)\n",
    "\n",
    "    return ate\n",
    "\n",
    "\n",
    "ate_maha = nn_matching_maha_ate(Y, Tr, X)\n",
    "print(f\"2. MAH (Mahalanobis Matching): ${ate_maha:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbe0e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. IPW (Inverse Propensity Weighting): $19.61\n"
     ]
    }
   ],
   "source": [
    "# Estimator 3: Inverse Propensity Weighting (IPW)\n",
    "def ipw_estimator(Y, Tr, X):\n",
    "    \"\"\"\n",
    "    IPW estimator following the R code\n",
    "    \"\"\"\n",
    "    # Fit logit model for propensity score\n",
    "    temp_df = pd.DataFrame({\n",
    "        'treatment': Tr,\n",
    "        'ffs_q2': X[:, 0],\n",
    "        'ffs_q3': X[:, 1],\n",
    "        'ffs_q4': X[:, 2]\n",
    "    })\n",
    "    \n",
    "    logit_model = smf.logit('treatment ~ ffs_q2 + ffs_q3 + ffs_q4', data=temp_df).fit(disp=False)\n",
    "    ps = logit_model.predict(temp_df)\n",
    "    \n",
    "    # Clip propensity scores to avoid extreme weights\n",
    "    ps = ps.clip(0.01, 0.99)\n",
    "    \n",
    "    # Calculate IPW weights\n",
    "    ipw = np.where(Tr == 1, 1.0 / ps, 1.0 / (1.0 - ps))\n",
    "    \n",
    "    # Weighted means\n",
    "    mean_treated = np.average(Y[Tr == 1], weights=ipw[Tr == 1])\n",
    "    mean_control = np.average(Y[Tr == 0], weights=ipw[Tr == 0])\n",
    "    \n",
    "    ate = mean_treated - mean_control\n",
    "    \n",
    "    return ate\n",
    "\n",
    "ate_ipw = ipw_estimator(Y, Tr, X)\n",
    "print(f\"3. IPW (Inverse Propensity Weighting): ${ate_ipw:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327f0bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. OLS (Regression with Interactions): $19.61\n"
     ]
    }
   ],
   "source": [
    "# Estimator 4: OLS Regression\n",
    "def ols_estimator(Y, Tr, X):\n",
    "    \"\"\"\n",
    "    OLS regression following regression adjustment approach\n",
    "    Run separate regressions by treatment status, then predict for everyone\n",
    "    \"\"\"\n",
    "    # Create dataframe for regression\n",
    "    reg_df = pd.DataFrame({\n",
    "        'y': Y,\n",
    "        'treatment': Tr,\n",
    "        'ffs_q2': X[:, 0],\n",
    "        'ffs_q3': X[:, 1],\n",
    "        'ffs_q4': X[:, 2]\n",
    "    })\n",
    "    \n",
    "    # Create interactions\n",
    "    reg_df['treat_q2'] = reg_df['treatment'] * reg_df['ffs_q2']\n",
    "    reg_df['treat_q3'] = reg_df['treatment'] * reg_df['ffs_q3']\n",
    "    reg_df['treat_q4'] = reg_df['treatment'] * reg_df['ffs_q4']\n",
    "    \n",
    "    # Run regression with interactions (allows separate slopes by treatment)\n",
    "    ols_model = smf.ols(\n",
    "        'y ~ treatment + ffs_q2 + ffs_q3 + ffs_q4 + treat_q2 + treat_q3 + treat_q4',\n",
    "        data=reg_df\n",
    "    ).fit()\n",
    "    \n",
    "    # Predict outcome for everyone AS IF treated\n",
    "    reg_df_1 = reg_df.copy()\n",
    "    reg_df_1['treatment'] = 1\n",
    "    reg_df_1['treat_q2'] = reg_df_1['ffs_q2']\n",
    "    reg_df_1['treat_q3'] = reg_df_1['ffs_q3']\n",
    "    reg_df_1['treat_q4'] = reg_df_1['ffs_q4']\n",
    "    pred1 = ols_model.predict(reg_df_1)\n",
    "    \n",
    "    # Predict outcome for everyone AS IF control\n",
    "    reg_df_0 = reg_df.copy()\n",
    "    reg_df_0['treatment'] = 0\n",
    "    reg_df_0['treat_q2'] = 0\n",
    "    reg_df_0['treat_q3'] = 0\n",
    "    reg_df_0['treat_q4'] = 0\n",
    "    pred0 = ols_model.predict(reg_df_0)\n",
    "    \n",
    "    # ATE is average difference across all units\n",
    "    ate = np.mean(pred1 - pred0)\n",
    "    \n",
    "    return ate\n",
    "\n",
    "ate_ols = ols_estimator(Y, Tr, X)\n",
    "print(f\"4. OLS (Regression with Interactions): ${ate_ols:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b853028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TABLE 3: Average Treatment Effects\n",
      "High HHI vs. Low HHI (2018 Data)\n",
      "======================================================================\n",
      "Estimator       ATE\n",
      "      INV 15.950978\n",
      "      MAH 24.358274\n",
      "      IPW 19.607228\n",
      "      OLS 19.607228\n",
      "\n",
      "Range of estimates: $8.4073\n",
      "Standard deviation: $3.4468\n",
      "\n",
      "⚠ WARNING: Estimates differ substantially\n",
      "  This suggests missing data or different sample sizes across methods\n",
      "  Check for NaN values or inconsistent treatment assignment\n"
     ]
    }
   ],
   "source": [
    "# Table 3: Comparing the Estimators\n",
    "table3 = pd.DataFrame({\n",
    "    'Estimator': ['INV', 'MAH', 'IPW', 'OLS'],\n",
    "    'ATE': [ate_inv, ate_maha, ate_ipw, ate_ols]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLE 3: Average Treatment Effects\")\n",
    "print(\"High HHI vs. Low HHI (2018 Data)\")\n",
    "print(\"=\"*70)\n",
    "print(table3.to_string(index=False))\n",
    "\n",
    "# Check if estimates are identical\n",
    "max_diff = table3['ATE'].max() - table3['ATE'].min()\n",
    "std_ate = table3['ATE'].std()\n",
    "\n",
    "print(f\"\\nRange of estimates: ${max_diff:.4f}\")\n",
    "print(f\"Standard deviation: ${std_ate:.4f}\")\n",
    "\n",
    "if max_diff < 0.50:  # Within 50 cents\n",
    "    print(\"\\n✓ SUCCESS: All estimates are essentially identical (as expected)\")\n",
    "    print(\"  This confirms all methods are using the same data and treatment definition.\")\n",
    "else:\n",
    "    print(\"\\n⚠ WARNING: Estimates differ substantially\")\n",
    "    print(\"  This suggests missing data or different sample sizes across methods\")\n",
    "    print(\"  Check for NaN values or inconsistent treatment assignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b078321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DIAGNOSTICS\n",
      "======================================================================\n",
      "Sample size: 1976\n",
      "Treated (High HHI): 988\n",
      "Control (Low HHI): 988\n",
      "\n",
      "Missing values check:\n",
      "  Missing in bid: 0\n",
      "  Missing in treatment: 0\n",
      "  Missing in ffs_q2: 0\n",
      "  Missing in ffs_q3: 0\n",
      "  Missing in ffs_q4: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Sample size: {len(analysis_data)}\")\n",
    "print(f\"Treated (High HHI): {(Tr==1).sum()}\")\n",
    "print(f\"Control (Low HHI): {(Tr==0).sum()}\")\n",
    "print(f\"\\nMissing values check:\")\n",
    "print(f\"  Missing in bid: {analysis_data['bid'].isna().sum()}\")\n",
    "print(f\"  Missing in treatment: {analysis_data['treatment'].isna().sum()}\")\n",
    "print(f\"  Missing in ffs_q2: {analysis_data['ffs_q2'].isna().sum()}\")\n",
    "print(f\"  Missing in ffs_q3: {analysis_data['ffs_q3'].isna().sum()}\")\n",
    "print(f\"  Missing in ffs_q4: {analysis_data['ffs_q4'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55ea2608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Table saved to: ../../data/output/table3_question7.csv\n",
      "✓ Analysis data saved to: ../../../data/output/question7_analysis_data.csv\n",
      "\n",
      "======================================================================\n",
      "COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "table3.to_csv(\"../../../data/output/table3_question7.csv\", index=False)\n",
    "print(\"\\n✓ Table saved to: ../../data/output/table3_question7.csv\")\n",
    "\n",
    "# Save the full analysis dataset \n",
    "analysis_data.to_csv(\"../../../data/output/question7_analysis_data.csv\", index=False)\n",
    "print(\"✓ Analysis data saved to: ../../../data/output/question7_analysis_data.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2f0e7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Redoing Question 9\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUESTION 9: OLS Estimator with Continuous Covariates\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Using OLS \n",
    "# Prepare data with continuous covariates\n",
    "continuous_data = analysis_data.dropna(subset=['bid', 'treatment', 'avg_ffscost', 'total_benef']).copy()\n",
    "\n",
    "print(f\"\\nSample size with continuous covariates: {len(continuous_data)}\")\n",
    "print(f\"Treated: {(continuous_data['treatment']==1).sum()}\")\n",
    "print(f\"Control: {(continuous_data['treatment']==0).sum()}\")\n",
    "\n",
    "# Define variables\n",
    "Y_cont = continuous_data['bid'].values\n",
    "Tr_cont = continuous_data['treatment'].values\n",
    "\n",
    "# Create dataframe for regression\n",
    "reg_df_cont = pd.DataFrame({\n",
    "    'y': Y_cont,\n",
    "    'treatment': Tr_cont,\n",
    "    'avg_ffscost': continuous_data['avg_ffscost'].values,\n",
    "    'total_benef': continuous_data['total_benef'].values\n",
    "})\n",
    "\n",
    "# Create interactions between treatment and continuous covariates\n",
    "reg_df_cont['treat_ffs'] = reg_df_cont['treatment'] * reg_df_cont['avg_ffscost']\n",
    "reg_df_cont['treat_benef'] = reg_df_cont['treatment'] * reg_df_cont['total_benef']\n",
    "\n",
    "# Run OLS with continuous covariates and interactions\n",
    "ols_continuous = smf.ols(\n",
    "    'y ~ treatment + avg_ffscost + total_benef + treat_ffs + treat_benef',\n",
    "    data=reg_df_cont\n",
    ").fit()\n",
    "\n",
    "print(\"\\nOLS Regression Results (Continuous Covariates):\")\n",
    "print(ols_continuous.summary().tables[1])\n",
    "\n",
    "# Calculate ATE by predicting for everyone as treated vs control\n",
    "# Predict as treated\n",
    "reg_df_1 = reg_df_cont.copy()\n",
    "reg_df_1['treatment'] = 1\n",
    "reg_df_1['treat_ffs'] = reg_df_1['avg_ffscost']\n",
    "reg_df_1['treat_benef'] = reg_df_1['total_benef']\n",
    "pred1_cont = ols_continuous.predict(reg_df_1)\n",
    "\n",
    "# Predict as control\n",
    "reg_df_0 = reg_df_cont.copy()\n",
    "reg_df_0['treatment'] = 0\n",
    "reg_df_0['treat_ffs'] = 0\n",
    "reg_df_0['treat_benef'] = 0\n",
    "pred0_cont = ols_continuous.predict(reg_df_0)\n",
    "\n",
    "# ATE is average difference\n",
    "ate_ols_continuous = np.mean(pred1_cont - pred0_cont)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON: Quartile-based vs Continuous Specification\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nOLS with FFS Quartiles:              ${ate_ols:.2f}\")\n",
    "print(f\"OLS with Continuous FFS + Beneficiaries: ${ate_ols_continuous:.2f}\")\n",
    "print(f\"\\nDifference:                           ${ate_ols_continuous - ate_ols:.2f}\")\n",
    "\n",
    "# Calculate percentage difference\n",
    "pct_diff = ((ate_ols_continuous - ate_ols) / ate_ols) * 100\n",
    "print(f\"Percentage difference:                 {pct_diff:.1f}%\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_table = pd.DataFrame({\n",
    "    'Specification': [\n",
    "        'Quartile-based (Q2, Q3, Q4)',\n",
    "        'Continuous (FFS costs + Total beneficiaries)'\n",
    "    ],\n",
    "    'ATE': [ate_ols, ate_ols_continuous],\n",
    "    'Covariates': [\n",
    "        'FFS quartile dummies',\n",
    "        'Continuous FFS costs, Total Medicare beneficiaries'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Table: OLS Estimates with Different Covariate Specifications\")\n",
    "print(f\"{'='*70}\")\n",
    "print(comparison_table.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "comparison_table.to_csv(\n",
    "    os.path.join(base_path, \"data\", \"output\", \"question9_comparison.csv\"),\n",
    "    index=False\n",
    ")\n",
    "print(f\"\\n✓ Comparison table saved to: data/output/question9_comparison.csv\")\n",
    "\n",
    "# Diagnostic: Check if adding total beneficiaries improves fit\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Model Diagnostics\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"R-squared (continuous model): {ols_continuous.rsquared:.4f}\")\n",
    "\n",
    "# For comparison, run model without beneficiaries\n",
    "ols_ffs_only = smf.ols(\n",
    "    'y ~ treatment + avg_ffscost + treat_ffs',\n",
    "    data=reg_df_cont\n",
    ").fit()\n",
    "print(f\"R-squared (FFS only):        {ols_ffs_only.rsquared:.4f}\")\n",
    "\n",
    "# Check coefficient on treatment\n",
    "print(f\"\\nTreatment coefficient (continuous model): ${ols_continuous.params['treatment']:.2f}\")\n",
    "print(f\"Standard error:                           ${ols_continuous.bse['treatment']:.2f}\")\n",
    "print(f\"P-value:                                  {ols_continuous.pvalues['treatment']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a1524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
